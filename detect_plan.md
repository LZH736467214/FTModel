# é‡‘èæ¨ç†æ¨¡å‹åè®­ç»ƒå®Œæ•´æ‰§è¡Œè®¡åˆ’ï¼ˆé¢è¯•å¯¼å‘ç‰ˆï¼‰

> **ç›®æ ‡**ï¼šä¸€å¤©å†…å®Œå…¨å­¦ä¼š"æ•°æ®æ„å»º â†’ SFT â†’ RL(GRPO) â†’ è¯„æµ‹ â†’ éƒ¨ç½²"å…¨é“¾è·¯
> **çº¦æŸ**ï¼šæœ¬åœ° 4070 8GB + AutoDL 5090 32GBï¼Œé¢„ç®— 100 å…ƒ
> **æ ¸å¿ƒ**ï¼šæœ¬åœ°æ„å»ºæ‰€æœ‰è„šæœ¬ï¼ŒæœåŠ¡å™¨çº¯æ‰§è¡Œè®­ç»ƒï¼Œå‡†å¤‡é¢è¯•ææ–™

---

## ğŸ“‹ ç›®å½•

1. [æ€»ä½“æ¶æ„](#æ€»ä½“æ¶æ„)
2. [é˜¶æ®µ0ï¼šé¡¹ç›®åˆå§‹åŒ–](#é˜¶æ®µ0é¡¹ç›®åˆå§‹åŒ–)
3. [é˜¶æ®µ1ï¼šæ•°æ®æ„å»ºæµæ°´çº¿](#é˜¶æ®µ1æ•°æ®æ„å»ºæµæ°´çº¿)
4. [é˜¶æ®µ2ï¼šè®­ç»ƒè„šæœ¬å¼€å‘](#é˜¶æ®µ2è®­ç»ƒè„šæœ¬å¼€å‘)
5. [é˜¶æ®µ3ï¼šæœåŠ¡å™¨è®­ç»ƒæ‰§è¡Œ](#é˜¶æ®µ3æœåŠ¡å™¨è®­ç»ƒæ‰§è¡Œ)
6. [é˜¶æ®µ4ï¼šè¯„æµ‹ä¸éƒ¨ç½²](#é˜¶æ®µ4è¯„æµ‹ä¸éƒ¨ç½²)
7. [é˜¶æ®µ5ï¼šé¢è¯•å‡†å¤‡](#é˜¶æ®µ5é¢è¯•å‡†å¤‡)
8. [å¸¸è§é—®é¢˜æ’æŸ¥](#å¸¸è§é—®é¢˜æ’æŸ¥)
9. [é¢è¯•é—®ç­”æ‰‹å†Œ](#é¢è¯•é—®ç­”æ‰‹å†Œ)

---

## æ€»ä½“æ¶æ„

### æ ¸å¿ƒæ€è·¯

åŸºäº Fin-R1 è®ºæ–‡çš„å·¥ç¨‹å®è·µï¼š

```
åŸå§‹æ•°æ® â†’ æ•™å¸ˆè’¸é¦ â†’ åŒé‡è¿‡æ»¤ â†’ è®­ç»ƒæ•°æ®èµ„äº§
                                      â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                              â†“               â†“
                            SFT            GRPO
                         (å­¦æ ¼å¼)        (æå‡†ç¡®ç‡)
                              â†“               â†“
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                              è¯„æµ‹ â†’ éƒ¨ç½² â†’ é¢è¯•
```

### å…³é”®è®¾è®¡å†³ç­–ï¼ˆé¢è¯•è¦ç‚¹ï¼‰

| é—®é¢˜ | å†³ç­– | åŸå›  |
|------|------|------|
| ä¸ºä»€ä¹ˆè¦æ•°æ®è’¸é¦ï¼Ÿ | ç”¨å¼ºæ¨¡å‹ç”Ÿæˆ CoT | å°æ¨¡å‹ç¼ºä¹æ¨ç†èƒ½åŠ› |
| ä¸ºä»€ä¹ˆåŒé‡è¿‡æ»¤ï¼Ÿ | ç­”æ¡ˆæ­£ç¡®æ€§ + æ¨ç†è´¨é‡ | ç­”æ¡ˆæ˜¯ç¡¬çº¦æŸï¼Œæ¨ç†æ˜¯è´¨é‡ä¿è¯ |
| ä¸ºä»€ä¹ˆå…ˆ SFTï¼Ÿ | å­¦ä¹ è¾“å‡ºæ ¼å¼ | RL éœ€è¦ç¨³å®šçš„æ ¼å¼ä½œä¸ºåŸºç¡€ |
| ä¸ºä»€ä¹ˆç”¨ GRPOï¼Ÿ | å¯éªŒè¯å¥–åŠ± + ç¨³å®šè®­ç»ƒ | é‡‘èåœºæ™¯éœ€è¦å®¢è§‚æ ‡å‡† |
| ä¸ºä»€ä¹ˆç”¨ vLLMï¼Ÿ | é«˜æ€§èƒ½æ¨ç† | PagedAttention + Continuous batching |

---

## é˜¶æ®µ0ï¼šé¡¹ç›®åˆå§‹åŒ–

### åˆ›å»ºé¡¹ç›®ç»“æ„

```bash
cd c:\gitclones\FTModel

# åˆ›å»ºç›®å½•
mkdir -p data/{raw,processed} scripts ckpts reports configs logs

# åˆ›å»º .gitignore
cat > .gitignore << 'EOF'
# æ¨¡å‹æƒé‡
ckpts/
*.bin
*.safetensors

# æ•°æ®æ–‡ä»¶
data/
!data/.gitkeep

# æ—¥å¿—
logs/
*.log

# Python
__pycache__/
*.pyc
.env

# IDE
.vscode/
.idea/
EOF

# åˆ›å»º README
cat > README.md << 'EOF'
# é‡‘èæ¨ç†æ¨¡å‹åè®­ç»ƒé¡¹ç›®

åŸºäº Fin-R1 è®ºæ–‡çš„å®Œæ•´åè®­ç»ƒé“¾è·¯å®ç°ã€‚

## å¿«é€Ÿå¼€å§‹

1. æœ¬åœ°æ•°æ®å‡†å¤‡ï¼š`python scripts/1_prepare_raw_data.py`
2. æœåŠ¡å™¨è®­ç»ƒï¼šè§ `å®Œæ•´æ‰§è¡Œè®¡åˆ’_é¢è¯•å¯¼å‘ç‰ˆ.md`

## é¡¹ç›®ç»“æ„

- `scripts/`: æ‰€æœ‰è®­ç»ƒå’Œè¯„æµ‹è„šæœ¬
- `data/`: æ•°æ®èµ„äº§ï¼ˆåŸå§‹ã€è’¸é¦ã€è¿‡æ»¤åï¼‰
- `ckpts/`: æ¨¡å‹æ£€æŸ¥ç‚¹
- `reports/`: è¯„æµ‹æŠ¥å‘Š
- `configs/`: é…ç½®æ–‡ä»¶
EOF
```

### åˆ›å»ºä¾èµ–æ–‡ä»¶

```bash
cat > requirements.txt << 'EOF'
# æ ¸å¿ƒæ¡†æ¶
torch==2.1.0
transformers==4.45.0
trl==0.12.0
peft==0.13.0

# é‡åŒ–å’ŒåŠ é€Ÿ
bitsandbytes==0.44.0
accelerate==0.34.0

# éƒ¨ç½²
vllm==0.6.3

# æ•°æ®å¤„ç†
datasets==2.20.0
pandas==2.1.0

# API è°ƒç”¨
openai==1.35.0
requests==2.31.0

# å·¥å…·
tqdm==4.66.0
wandb  # å¯é€‰ï¼Œç”¨äºè®­ç»ƒç›‘æ§

# Qwen ç‰¹å®š
sentencepiece==0.2.0
protobuf==4.25.0
EOF
```

### åˆ›å»ºé…ç½®æ–‡ä»¶

```python
# configs/config.py
"""
å…¨å±€é…ç½®æ–‡ä»¶
é¢è¯•ç‚¹ï¼šé…ç½®ç®¡ç†çš„æœ€ä½³å®è·µ
"""
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    base_model: str = "Qwen/Qwen2.5-3B-Instruct"
    model_max_length: int = 2048

@dataclass
class LoRAConfig:
    """LoRA é…ç½®"""
    r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: list = None

    def __post_init__(self):
        if self.target_modules is None:
            # Qwen ç³»åˆ—çš„ LoRA ç›®æ ‡æ¨¡å—
            self.target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]

@dataclass
class SFTConfig:
    """SFT è®­ç»ƒé…ç½®"""
    output_dir: str = "ckpts/sft_lora"
    num_train_epochs: int = 2
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 8
    learning_rate: float = 2e-4
    lr_scheduler_type: str = "cosine"
    warmup_ratio: float = 0.05
    logging_steps: int = 10
    save_steps: int = 100
    save_total_limit: int = 3
    bf16: bool = True
    optim: str = "paged_adamw_8bit"

@dataclass
class GRPOConfig:
    """GRPO è®­ç»ƒé…ç½®"""
    output_dir: str = "ckpts/grpo_lora"
    num_train_epochs: int = 1
    per_device_train_batch_size: int = 1
    gradient_accumulation_steps: int = 16
    learning_rate: float = 5e-6
    num_sample_generations: int = 4
    response_length: int = 512
    temperature: float = 0.7
    kl_coef: float = 0.05
    format_reward_weight: float = 0.3
    accuracy_reward_weight: float = 0.7

@dataclass
class APIConfig:
    """API é…ç½®"""
    provider: str = "deepseek"  # deepseek, qwen, openai
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: Optional[str] = None

    def __post_init__(self):
        # ä»ç¯å¢ƒå˜é‡è¯»å–
        if self.api_key is None:
            self.api_key = os.getenv("API_KEY")

        # æ ¹æ® provider è®¾ç½®é»˜è®¤å€¼
        if self.provider == "deepseek":
            self.base_url = self.base_url or "https://api.deepseek.com/v1"
            self.model_name = self.model_name or "deepseek-chat"
        elif self.provider == "qwen":
            self.base_url = self.base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            self.model_name = self.model_name or "qwen-max"

@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    raw_data_path: str = "data/raw/raw.jsonl"
    distilled_data_path: str = "data/processed/distilled.jsonl"
    sft_data_path: str = "data/processed/sft.jsonl"
    rl_data_path: str = "data/processed/rl.jsonl"
    test_data_path: str = "data/processed/test.jsonl"

    # æ•°æ®è§„æ¨¡
    total_samples: int = 500
    test_ratio: float = 0.2

    # åˆ†å±‚æŠ½æ ·é…ç½®
    type_distribution: dict = None

    def __post_init__(self):
        if self.type_distribution is None:
            self.type_distribution = {
                "financial_calculation": 0.4,
                "business_reasoning": 0.3,
                "concept_qa": 0.2,
                "risk_analysis": 0.1
            }

# å…¨å±€é…ç½®å®ä¾‹
MODEL_CONFIG = ModelConfig()
LORA_CONFIG = LoRAConfig()
SFT_CONFIG = SFTConfig()
GRPO_CONFIG = GRPOConfig()
API_CONFIG = APIConfig()
DATA_CONFIG = DataConfig()
```

---

## é˜¶æ®µ1ï¼šæ•°æ®æ„å»ºæµæ°´çº¿

### 1.1 å‡†å¤‡åŸå§‹æ•°æ®

```python
# scripts/1_prepare_raw_data.py
"""
ä» qwen-dianjin å’Œè‡ªå®šä¹‰æ•°æ®ä¸­å‡†å¤‡åŸå§‹æ•°æ®é›†
è¾“å‡ºï¼šdata/raw/raw.jsonl
"""
import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))
from configs.config import DATA_CONFIG

def download_qwen_dianjin():
    """
    ä¸‹è½½ qwen-dianjin æ•°æ®é›†
    é¢è¯•ç‚¹ï¼šæ•°æ®æ¥æºçš„å¤šæ ·æ€§
    """
    try:
        # æ–¹æ³•1ï¼šä» ModelScope ä¸‹è½½ï¼ˆå›½å†…é€Ÿåº¦å¿«ï¼‰
        from modelscope.msdatasets import MsDataset
        ds = MsDataset.load('qwen/qwen-dianjin', split='train')
        return list(ds)
    except:
        pass

    try:
        # æ–¹æ³•2ï¼šä» HuggingFace ä¸‹è½½
        from datasets import load_dataset
        ds = load_dataset("Qwen/Qwen-Dianjin", split="train")
        return list(ds)
    except:
        pass

    # æ–¹æ³•3ï¼šæ‰‹åŠ¨ä¸‹è½½æç¤º
    print("âš ï¸  æ— æ³•è‡ªåŠ¨ä¸‹è½½ qwen-dianjin æ•°æ®é›†")
    print("è¯·æ‰‹åŠ¨ä¸‹è½½ï¼š")
    print("  1. è®¿é—® https://github.com/QwenLM/Qwen-Dianjin")
    print("  2. ä¸‹è½½æ•°æ®é›†å¹¶æ”¾åˆ° data/raw/qwen-dianjin.jsonl")

    # å°è¯•è¯»å–æœ¬åœ°æ–‡ä»¶
    local_path = "data/raw/qwen-dianjin.jsonl"
    if os.path.exists(local_path):
        with open(local_path, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]

    return []

def classify_question_type(question, answer=""):
    """
    æ ¹æ®é—®é¢˜å†…å®¹åˆ†ç±»
    é¢è¯•ç‚¹ï¼šæ•°æ®åˆ†ç±»çš„å¯å‘å¼è§„åˆ™
    """
    question_lower = question.lower()

    # é‡‘èè®¡ç®—é¢˜ç‰¹å¾
    calc_keywords = ["è®¡ç®—", "å¢é•¿ç‡", "æ”¶ç›Šç‡", "å¸‚ç›ˆç‡", "å¤šå°‘", "ç™¾åˆ†ä¹‹",
                     "åŒæ¯”", "ç¯æ¯”", "åˆ©æ¶¦", "è¥æ”¶", "è‚¡ä»·"]
    if any(kw in question for kw in calc_keywords):
        return "financial_calculation"

    # æ¦‚å¿µé—®ç­”é¢˜ç‰¹å¾
    concept_keywords = ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "æ¦‚å¿µ", "å«ä¹‰", "è§£é‡Š"]
    if any(kw in question for kw in concept_keywords):
        return "concept_qa"

    # é£é™©åˆ†æé¢˜ç‰¹å¾
    risk_keywords = ["é£é™©", "å½±å“", "åæœ", "é¢„æµ‹", "è¶‹åŠ¿"]
    if any(kw in question for kw in risk_keywords):
        return "risk_analysis"

    # é»˜è®¤ä¸ºä¸šåŠ¡æ¨ç†
    return "business_reasoning"

def create_custom_data():
    """
    åˆ›å»ºè‡ªå®šä¹‰æ•°æ®
    é¢è¯•ç‚¹ï¼šå±•ç¤ºæ•°æ®æ„é€ èƒ½åŠ›
    """
    custom_samples = [
        {
            "question": "æŸå…¬å¸2023å¹´è¥æ”¶1000ä¸‡å…ƒï¼ŒåŒæ¯”å¢é•¿25%ï¼Œè¯·è®¡ç®—2022å¹´è¥æ”¶ã€‚",
            "gold_answer": "800",
            "type": "financial_calculation",
            "source": "custom",
            "explanation": "2023å¹´è¥æ”¶ / (1 + å¢é•¿ç‡) = 1000 / 1.25 = 800ä¸‡å…ƒ"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯èµ„äº§è´Ÿå€ºç‡ï¼Ÿå¦‚ä½•è®¡ç®—ï¼Ÿ",
            "gold_answer": "èµ„äº§è´Ÿå€ºç‡ = (æ€»è´Ÿå€º / æ€»èµ„äº§) Ã— 100%ï¼Œç”¨äºè¡¡é‡ä¼ä¸šé•¿æœŸå¿å€ºèƒ½åŠ›ã€‚",
            "type": "concept_qa",
            "source": "custom"
        },
        {
            "question": "æŸè‚¡ç¥¨å½“å‰ä»·æ ¼50å…ƒï¼Œå¸‚ç›ˆç‡20ï¼Œè¯·è®¡ç®—è¯¥å…¬å¸æ¯è‚¡æ”¶ç›Šã€‚",
            "gold_answer": "2.5",
            "type": "financial_calculation",
            "source": "custom",
            "explanation": "æ¯è‚¡æ”¶ç›Š = è‚¡ä»· / å¸‚ç›ˆç‡ = 50 / 20 = 2.5å…ƒ"
        },
        {
            "question": "å¤®è¡Œæé«˜åˆ©ç‡å¯¹è‚¡å¸‚æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ",
            "gold_answer": "å¤®è¡Œæé«˜åˆ©ç‡é€šå¸¸ä¼šå¯¼è‡´è‚¡å¸‚ä¸‹è·Œï¼Œå› ä¸ºï¼š1ï¼‰èèµ„æˆæœ¬ä¸Šå‡ï¼Œä¼ä¸šç›ˆåˆ©ä¸‹é™ï¼›2ï¼‰å€ºåˆ¸ç­‰å›ºå®šæ”¶ç›Šäº§å“å¸å¼•åŠ›å¢åŠ ï¼›3ï¼‰å¸‚åœºæµåŠ¨æ€§æ”¶ç´§ã€‚",
            "type": "business_reasoning",
            "source": "custom"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯æµåŠ¨æ¯”ç‡ï¼Ÿæ­£å¸¸èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ",
            "gold_answer": "æµåŠ¨æ¯”ç‡ = æµåŠ¨èµ„äº§ / æµåŠ¨è´Ÿå€ºï¼Œæ­£å¸¸èŒƒå›´ä¸º1.5-2.0ï¼Œç”¨äºè¡¡é‡ä¼ä¸šçŸ­æœŸå¿å€ºèƒ½åŠ›ã€‚",
            "type": "concept_qa",
            "source": "custom"
        },
    ]

    return custom_samples

def stratified_sampling(data_source, target_count, type_distribution):
    """
    åˆ†å±‚æŠ½æ ·
    é¢è¯•ç‚¹ï¼šå¦‚ä½•ä¿è¯æ•°æ®åˆ†å¸ƒåˆç†
    """
    # æŒ‰ç±»å‹åˆ†ç»„
    type_buckets = {}
    for item in data_source:
        qtype = item.get("type")
        if qtype not in type_buckets:
            type_buckets[qtype] = []
        type_buckets[qtype].append(item)

    # æŒ‰æ¯”ä¾‹æŠ½æ ·
    sampled_data = []
    for qtype, ratio in type_distribution.items():
        target_n = int(target_count * ratio)
        bucket = type_buckets.get(qtype, [])

        if len(bucket) >= target_n:
            sampled_data.extend(bucket[:target_n])
        else:
            # ä¸å¤Ÿå°±å…¨éƒ¨åŠ å…¥
            sampled_data.extend(bucket)
            print(f"âš ï¸  {qtype} åªæœ‰ {len(bucket)} æ¡ï¼Œå°‘äºç›®æ ‡ {target_n} æ¡")

    return sampled_data

def prepare_raw_data():
    """ä¸»æµç¨‹"""
    print("="*60)
    print("é˜¶æ®µ1ï¼šå‡†å¤‡åŸå§‹æ•°æ®")
    print("="*60)

    # åˆ›å»ºç›®å½•
    os.makedirs("data/raw", exist_ok=True)

    all_data = []

    # 1. åŠ è½½ qwen-dianjin
    print("\n1. åŠ è½½ qwen-dianjin æ•°æ®é›†...")
    qwen_data = download_qwen_dianjin()

    if qwen_data:
        print(f"   âœ“ åŠ è½½æˆåŠŸï¼š{len(qwen_data)} æ¡")

        # è½¬æ¢æ ¼å¼å¹¶åˆ†ç±»
        for idx, item in enumerate(qwen_data[:400]):  # åªå–å‰400æ¡
            # qwen-dianjin çš„æ•°æ®æ ¼å¼å¯èƒ½æ˜¯ {input, target} æˆ– {question, answer}
            question = item.get("input") or item.get("question", "")
            answer = item.get("target") or item.get("answer", "")

            if question and answer:
                qtype = classify_question_type(question, answer)
                all_data.append({
                    "id": f"qwen_{idx}",
                    "question": question,
                    "gold_answer": answer,
                    "type": qtype,
                    "source": "qwen-dianjin"
                })
    else:
        print("   âœ— åŠ è½½å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®")

    # 2. æ·»åŠ è‡ªå®šä¹‰æ•°æ®
    print("\n2. æ·»åŠ è‡ªå®šä¹‰æ•°æ®...")
    custom_data = create_custom_data()
    for idx, item in enumerate(custom_data):
        item["id"] = f"custom_{idx}"
        all_data.append(item)
    print(f"   âœ“ æ·»åŠ  {len(custom_data)} æ¡è‡ªå®šä¹‰æ•°æ®")

    # 3. åˆ†å±‚æŠ½æ ·ï¼ˆå¦‚æœæ•°æ®é‡è¶…è¿‡ç›®æ ‡ï¼‰
    if len(all_data) > DATA_CONFIG.total_samples:
        print(f"\n3. åˆ†å±‚æŠ½æ ·è‡³ {DATA_CONFIG.total_samples} æ¡...")
        all_data = stratified_sampling(
            all_data,
            DATA_CONFIG.total_samples,
            DATA_CONFIG.type_distribution
        )

    # 4. ç»Ÿè®¡ä¿¡æ¯
    type_counts = {}
    for item in all_data:
        qtype = item["type"]
        type_counts[qtype] = type_counts.get(qtype, 0) + 1

    print("\n" + "="*60)
    print("æ•°æ®èµ„äº§ç»Ÿè®¡")
    print("="*60)
    print(f"æ€»æ•°: {len(all_data)}")
    for qtype, count in sorted(type_counts.items()):
        print(f"  {qtype:25s}: {count:3d} ({count/len(all_data)*100:5.1f}%)")
    print("="*60)

    # 5. ä¿å­˜
    output_path = DATA_CONFIG.raw_data_path
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        for item in all_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"\nâœ… åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: {output_path}")

    return len(all_data)

if __name__ == "__main__":
    count = prepare_raw_data()
    print(f"\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python scripts/2_distill_data.py")
```

### 1.2 æ•°æ®è’¸é¦

```python
# scripts/2_distill_data.py
"""
ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆ CoT
è¾“å‡ºï¼šdata/processed/distilled.jsonl
"""
import json
import os
import sys
from pathlib import Path
from tqdm import tqdm
import time

sys.path.insert(0, str(Path(__file__).parent.parent))
from configs.config import DATA_CONFIG, API_CONFIG

def get_api_client():
    """
    è·å– API å®¢æˆ·ç«¯
    é¢è¯•ç‚¹ï¼šAPI è°ƒç”¨çš„é€šç”¨å°è£…
    """
    from openai import OpenAI

    if not API_CONFIG.api_key:
        print("âš ï¸  æœªè®¾ç½® API_KEY")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport API_KEY=your_api_key")
        print("æˆ–åœ¨ configs/config.py ä¸­é…ç½®")
        sys.exit(1)

    client = OpenAI(
        api_key=API_CONFIG.api_key,
        base_url=API_CONFIG.base_url
    )

    return client

def create_distillation_prompt(question, qtype):
    """
    æ„é€ è’¸é¦ prompt
    é¢è¯•ç‚¹ï¼šprompt å·¥ç¨‹çš„é‡è¦æ€§
    """
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªé‡‘èé¢†åŸŸä¸“å®¶ã€‚è¯·ç”¨ä»¥ä¸‹ä¸¥æ ¼æ ¼å¼å›ç­”é—®é¢˜ï¼š

<think>
[è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œå¿…é¡»åŒ…å«3-5ä¸ªæ¸…æ™°çš„æ¨ç†æ­¥éª¤]
</think>
<answer>
[æœ€ç»ˆç­”æ¡ˆï¼Œç®€æ´æ˜ç¡®]
</answer>

è¦æ±‚ï¼š
1. <think> éƒ¨åˆ†å¿…é¡»å±•ç¤ºå®Œæ•´æ¨ç†é€»è¾‘ï¼š
   - é‡‘èè®¡ç®—é¢˜ï¼šå†™å‡ºå…¬å¼ã€ä»£å…¥æ•°å€¼ã€è®¡ç®—è¿‡ç¨‹
   - æ¦‚å¿µé¢˜ï¼šå®šä¹‰ â†’ ç»„æˆè¦ç´  â†’ è®¡ç®—æ–¹æ³•/åº”ç”¨åœºæ™¯
   - åˆ†æé¢˜ï¼šç°è±¡ â†’ åŸå› åˆ†æ â†’ å½±å“/ç»“è®º
2. æ¨ç†æ­¥éª¤ç”¨"é¦–å…ˆ"ã€"å…¶æ¬¡"ã€"ç„¶å"ã€"å› æ­¤"ç­‰è¿æ¥è¯
3. <answer> éƒ¨åˆ†åªåŒ…å«æœ€ç»ˆç­”æ¡ˆï¼š
   - æ•°å€¼é¢˜ï¼šç›´æ¥ç»™å‡ºæ•°å­—ï¼ˆä¸è¦å•ä½ï¼‰
   - æ¦‚å¿µ/åˆ†æé¢˜ï¼š1-2å¥è¯çš„ç®€æ´ç­”æ¡ˆ
4. ä¸¥æ ¼éµå®ˆæ ‡ç­¾æ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å¤šä½™å†…å®¹"""

    user_prompt = f"é—®é¢˜ï¼š{question}"

    return system_prompt, user_prompt

def call_teacher_model(client, question, qtype, max_retries=3):
    """
    è°ƒç”¨æ•™å¸ˆæ¨¡å‹
    é¢è¯•ç‚¹ï¼šé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    """
    system_prompt, user_prompt = create_distillation_prompt(question, qtype)

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=API_CONFIG.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.6,
                max_tokens=1024
            )

            output = response.choices[0].message.content
            return output, None

        except Exception as e:
            error_msg = str(e)
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"   âš ï¸  é‡è¯• {attempt+1}/{max_retries}ï¼ˆç­‰å¾… {wait_time}sï¼‰: {error_msg[:50]}")
                time.sleep(wait_time)
            else:
                return None, error_msg

    return None, "Max retries exceeded"

def distill_data():
    """ä¸»æµç¨‹"""
    print("="*60)
    print("é˜¶æ®µ2ï¼šæ•°æ®è’¸é¦ï¼ˆTeacher ç”Ÿæˆ CoTï¼‰")
    print("="*60)

    # åŠ è½½åŸå§‹æ•°æ®
    print(f"\nåŠ è½½åŸå§‹æ•°æ®: {DATA_CONFIG.raw_data_path}")
    with open(DATA_CONFIG.raw_data_path, 'r', encoding='utf-8') as f:
        raw_data = [json.loads(line) for line in f]
    print(f"âœ“ åŠ è½½ {len(raw_data)} æ¡")

    # åˆå§‹åŒ– API
    print(f"\nåˆå§‹åŒ– API: {API_CONFIG.provider}")
    client = get_api_client()
    print(f"âœ“ ä½¿ç”¨æ¨¡å‹: {API_CONFIG.model_name}")

    # è’¸é¦
    distilled_data = []
    failed_items = []

    print(f"\nå¼€å§‹è’¸é¦...")
    for item in tqdm(raw_data, desc="è’¸é¦è¿›åº¦"):
        teacher_output, error = call_teacher_model(
            client,
            item["question"],
            item["type"]
        )

        if teacher_output:
            distilled_data.append({
                **item,
                "teacher_output": teacher_output
            })
        else:
            failed_items.append({
                "id": item["id"],
                "error": error
            })

        # æ¯10æ¡ä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰
        if len(distilled_data) % 10 == 0:
            temp_path = DATA_CONFIG.distilled_data_path + ".tmp"
            os.makedirs(os.path.dirname(temp_path), exist_ok=True)
            with open(temp_path, 'w', encoding='utf-8') as f:
                for d in distilled_data:
                    f.write(json.dumps(d, ensure_ascii=False) + '\n')

    # ä¿å­˜æœ€ç»ˆç»“æœ
    output_path = DATA_CONFIG.distilled_data_path
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        for item in distilled_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    # ä¿å­˜å¤±è´¥è®°å½•
    if failed_items:
        fail_path = "data/processed/distill_failures.jsonl"
        with open(fail_path, 'w', encoding='utf-8') as f:
            for item in failed_items:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

    # ç»Ÿè®¡
    print("\n" + "="*60)
    print("è’¸é¦ç»“æœ")
    print("="*60)
    print(f"æ€»æ•°: {len(raw_data)}")
    print(f"æˆåŠŸ: {len(distilled_data)} ({len(distilled_data)/len(raw_data)*100:.1f}%)")
    print(f"å¤±è´¥: {len(failed_items)} ({len(failed_items)/len(raw_data)*100:.1f}%)")
    print("="*60)

    print(f"\nâœ… è’¸é¦æ•°æ®å·²ä¿å­˜è‡³: {output_path}")

    # å±•ç¤ºä¸€ä¸ªæ ·ä¾‹
    if distilled_data:
        print("\n" + "="*60)
        print("æ ·ä¾‹å±•ç¤º")
        print("="*60)
        sample = distilled_data[0]
        print(f"é—®é¢˜: {sample['question']}")
        print(f"\næ•™å¸ˆè¾“å‡º:\n{sample['teacher_output']}")
        print("="*60)

    return len(distilled_data)

if __name__ == "__main__":
    count = distill_data()
    print(f"\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python scripts/3_filter_data.py")
```

### 1.3 åŒé‡è¿‡æ»¤

```python
# scripts/3_filter_data.py
"""
åŒé‡è¿‡æ»¤ï¼šç­”æ¡ˆæ­£ç¡®æ€§ + æ¨ç†è´¨é‡
è¾“å‡ºï¼šdata/processed/sft.jsonl, data/processed/rl.jsonl
"""
import json
import os
import sys
import re
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
from configs.config import DATA_CONFIG

class DataFilter:
    """
    æ•°æ®è¿‡æ»¤å™¨
    é¢è¯•ç‚¹ï¼šFin-R1 çš„è¿‡æ»¤ç­–ç•¥
    """

    def __init__(self):
        self.stats = {
            "total": 0,
            "format_ok": 0,
            "answer_correct": 0,
            "reasoning_good": 0,
            "final_pass": 0,
            "filter_reasons": {}
        }

    def extract_answer(self, text):
        """æå– <answer> ä¸­çš„å†…å®¹"""
        match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
        if not match:
            return None

        answer = match.group(1).strip()

        # å¤„ç† \boxed{} æ ¼å¼
        boxed_match = re.search(r'\\boxed\{([^}]+)\}', answer)
        if boxed_match:
            return boxed_match.group(1).strip()

        return answer

    def extract_think(self, text):
        """æå– <think> ä¸­çš„å†…å®¹"""
        match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)
        return match.group(1).strip() if match else None

    # ========== ç¬¬ä¸€å±‚ï¼šæ ¼å¼æ£€æŸ¥ ==========

    def check_format(self, text):
        """æ£€æŸ¥æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚"""
        think_count = text.count("<think>")
        answer_count = text.count("<answer>")

        if think_count != 1 or answer_count != 1:
            return False, "æ ‡ç­¾æ•°é‡ä¸å¯¹"

        # æ£€æŸ¥æ ‡ç­¾é¡ºåº
        think_pos = text.find("<think>")
        answer_pos = text.find("<answer>")

        if think_pos > answer_pos:
            return False, "æ ‡ç­¾é¡ºåºé”™è¯¯"

        return True, "OK"

    # ========== ç¬¬äºŒå±‚ï¼šç­”æ¡ˆæ­£ç¡®æ€§ ==========

    def check_answer_math(self, extracted, gold):
        """æ•°å­¦é¢˜ç­”æ¡ˆæ£€æŸ¥"""
        try:
            # æå–æ•°å­—
            extracted_num = float(re.sub(r'[^\d.-]', '', str(extracted)))
            gold_num = float(re.sub(r'[^\d.-]', '', str(gold)))

            # ç›¸å¯¹è¯¯å·®æˆ–ç»å¯¹è¯¯å·®
            if abs(gold_num) > 1:
                # ç›¸å¯¹è¯¯å·®
                return abs(extracted_num - gold_num) / abs(gold_num) < 0.01
            else:
                # ç»å¯¹è¯¯å·®
                return abs(extracted_num - gold_num) < 0.01
        except:
            return False

    def check_answer_qa(self, extracted, gold):
        """QA é¢˜ç­”æ¡ˆæ£€æŸ¥ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
        gold_lower = str(gold).lower()
        extracted_lower = str(extracted).lower()

        # ç›´æ¥åŒ…å«
        if gold_lower in extracted_lower:
            return True

        # å…³é”®è¯é‡å 
        gold_keywords = set(re.findall(r'[\w\u4e00-\u9fff]+', gold_lower))
        extracted_keywords = set(re.findall(r'[\w\u4e00-\u9fff]+', extracted_lower))

        if not gold_keywords:
            return False

        overlap_ratio = len(gold_keywords & extracted_keywords) / len(gold_keywords)
        return overlap_ratio > 0.6

    def check_answer_correctness(self, item):
        """ç­”æ¡ˆæ­£ç¡®æ€§æ€»å…¥å£"""
        extracted = self.extract_answer(item["teacher_output"])

        if not extracted:
            return False, "æ— æ³•æå–ç­”æ¡ˆ"

        gold = item["gold_answer"]
        qtype = item["type"]

        if qtype == "financial_calculation":
            is_correct = self.check_answer_math(extracted, gold)
        else:
            is_correct = self.check_answer_qa(extracted, gold)

        return is_correct, "OK" if is_correct else "ç­”æ¡ˆä¸æ­£ç¡®"

    # ========== ç¬¬ä¸‰å±‚ï¼šæ¨ç†è´¨é‡ ==========

    def check_reasoning_quality(self, item):
        """
        æ¨ç†è´¨é‡æ£€æŸ¥
        é¢è¯•ç‚¹ï¼šFin-R1 ç”¨ 7 ä¸ªç»´åº¦ï¼Œæˆ‘ä»¬ç®€åŒ–ä¸º 4 ä¸ªæ ¸å¿ƒç»´åº¦
        """
        think_text = self.extract_think(item["teacher_output"])

        if not think_text:
            return False, "ç¼ºå°‘æ¨ç†è¿‡ç¨‹"

        # ç»´åº¦1ï¼šé•¿åº¦åˆç†æ€§
        if len(think_text) < 50:
            return False, "æ¨ç†è¿‡ç¨‹è¿‡çŸ­"
        if len(think_text) > 2000:
            return False, "æ¨ç†è¿‡ç¨‹è¿‡é•¿"

        # ç»´åº¦2ï¼šé€»è¾‘è¿æ¥è¯
        reasoning_keywords = [
            "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æ¥ç€", "æœ€å",
            "å› æ­¤", "æ‰€ä»¥", "ç”±äº", "æ ¹æ®", "å¯å¾—",
            "è®¡ç®—", "æ¨å¯¼", "åˆ†æ", "å¾—å‡º", "ç»¼ä¸Š"
        ]
        keyword_count = sum(1 for kw in reasoning_keywords if kw in think_text)
        if keyword_count < 2:
            return False, "ç¼ºå°‘é€»è¾‘è¿æ¥è¯"

        # ç»´åº¦3ï¼šæ­¥éª¤æ¸…æ™°æ€§ï¼ˆé‡‘èè®¡ç®—é¢˜éœ€è¦æœ‰è®¡ç®—è¿‡ç¨‹ï¼‰
        if item["type"] == "financial_calculation":
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­¦ç¬¦å·
            if not re.search(r'[=\+\-\*/Ã·Ã—()]', think_text):
                return False, "è®¡ç®—é¢˜ç¼ºå°‘è®¡ç®—è¿‡ç¨‹"

        # ç»´åº¦4ï¼šé‡å¤å†…å®¹æ£€æŸ¥
        sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ\n]', think_text) if s.strip()]
        if len(sentences) > 3:
            unique_ratio = len(set(sentences)) / len(sentences)
            if unique_ratio < 0.7:
                return False, "å­˜åœ¨å¤§é‡é‡å¤å†…å®¹"

        return True, "OK"

    # ========== ä¸»æµç¨‹ ==========

    def filter_item(self, item):
        """è¿‡æ»¤å•æ¡æ•°æ®"""
        # ç¬¬ä¸€å±‚ï¼šæ ¼å¼
        format_ok, format_reason = self.check_format(item["teacher_output"])
        if not format_ok:
            self.stats["filter_reasons"][format_reason] = \
                self.stats["filter_reasons"].get(format_reason, 0) + 1
            return False

        self.stats["format_ok"] += 1

        # ç¬¬äºŒå±‚ï¼šç­”æ¡ˆ
        answer_ok, answer_reason = self.check_answer_correctness(item)
        if not answer_ok:
            self.stats["filter_reasons"][answer_reason] = \
                self.stats["filter_reasons"].get(answer_reason, 0) + 1
            return False

        self.stats["answer_correct"] += 1

        # ç¬¬ä¸‰å±‚ï¼šæ¨ç†è´¨é‡
        quality_ok, quality_reason = self.check_reasoning_quality(item)
        if not quality_ok:
            self.stats["filter_reasons"][quality_reason] = \
                self.stats["filter_reasons"].get(quality_reason, 0) + 1
            return False

        self.stats["reasoning_good"] += 1
        self.stats["final_pass"] += 1

        return True

    def filter_all(self, data):
        """è¿‡æ»¤æ‰€æœ‰æ•°æ®"""
        self.stats["total"] = len(data)

        sft_data = []
        rl_data = []

        for item in data:
            if self.filter_item(item):
                # SFT æ•°æ®
                sft_data.append({
                    "id": item["id"],
                    "prompt": item["question"],
                    "response": item["teacher_output"],
                    "type": item["type"]
                })

                # RL æ•°æ®
                rl_data.append({
                    "id": item["id"],
                    "prompt": item["question"],
                    "gold_answer": item["gold_answer"],
                    "type": item["type"]
                })

        return sft_data, rl_data

    def print_report(self):
        """æ‰“å°è¿‡æ»¤æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("æ•°æ®è¿‡æ»¤æŠ¥å‘Š")
        print("="*60)
        print(f"{'æŒ‡æ ‡':<20s} {'æ•°é‡':>10s} {'å æ¯”':>10s}")
        print("-"*60)
        print(f"{'åŸå§‹æ ·æœ¬':<20s} {self.stats['total']:>10d} {100.0:>9.1f}%")
        print(f"{'æ ¼å¼æ­£ç¡®':<20s} {self.stats['format_ok']:>10d} {self.stats['format_ok']/self.stats['total']*100:>9.1f}%")
        print(f"{'ç­”æ¡ˆæ­£ç¡®':<20s} {self.stats['answer_correct']:>10d} {self.stats['answer_correct']/self.stats['total']*100:>9.1f}%")
        print(f"{'æ¨ç†åˆæ ¼':<20s} {self.stats['reasoning_good']:>10d} {self.stats['reasoning_good']/self.stats['total']*100:>9.1f}%")
        print(f"{'æœ€ç»ˆé€šè¿‡':<20s} {self.stats['final_pass']:>10d} {self.stats['final_pass']/self.stats['total']*100:>9.1f}%")
        print("="*60)

        if self.stats["filter_reasons"]:
            print("\nè¿‡æ»¤åŸå› åˆ†å¸ƒ:")
            for reason, count in sorted(self.stats["filter_reasons"].items(),
                                       key=lambda x: -x[1]):
                print(f"  {reason:<30s}: {count:>5d}")

        print("="*60)

def split_train_test(sft_data, rl_data, test_ratio=0.2):
    """
    åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    é¢è¯•ç‚¹ï¼šå¦‚ä½•é¿å…æ•°æ®æ³„æ¼
    """
    import random
    random.seed(42)

    # æŒ‰ ID å¯¹é½
    assert len(sft_data) == len(rl_data)
    assert all(sft["id"] == rl["id"] for sft, rl in zip(sft_data, rl_data))

    # æ‰“ä¹±
    indices = list(range(len(sft_data)))
    random.shuffle(indices)

    # åˆ‡åˆ†
    test_size = int(len(sft_data) * test_ratio)
    test_indices = set(indices[:test_size])

    sft_train = [sft_data[i] for i in range(len(sft_data)) if i not in test_indices]
    sft_test = [sft_data[i] for i in range(len(sft_data)) if i in test_indices]

    rl_train = [rl_data[i] for i in range(len(rl_data)) if i not in test_indices]
    rl_test = [rl_data[i] for i in range(len(rl_data)) if i in test_indices]

    return sft_train, sft_test, rl_train, rl_test

def filter_data():
    """ä¸»æµç¨‹"""
    print("="*60)
    print("é˜¶æ®µ3ï¼šåŒé‡è¿‡æ»¤ï¼ˆç­”æ¡ˆ + æ¨ç†è´¨é‡ï¼‰")
    print("="*60)

    # åŠ è½½è’¸é¦æ•°æ®
    print(f"\nåŠ è½½è’¸é¦æ•°æ®: {DATA_CONFIG.distilled_data_path}")
    with open(DATA_CONFIG.distilled_data_path, 'r', encoding='utf-8') as f:
        distilled_data = [json.loads(line) for line in f]
    print(f"âœ“ åŠ è½½ {len(distilled_data)} æ¡")

    # è¿‡æ»¤
    print("\næ‰§è¡Œè¿‡æ»¤...")
    filter_obj = DataFilter()
    sft_data, rl_data = filter_obj.filter_all(distilled_data)

    # æ‰“å°æŠ¥å‘Š
    filter_obj.print_report()

    # åˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    print("\nåˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•é›†...")
    sft_train, sft_test, rl_train, rl_test = split_train_test(
        sft_data, rl_data, DATA_CONFIG.test_ratio
    )

    print(f"SFT è®­ç»ƒé›†: {len(sft_train)} æ¡")
    print(f"SFT æµ‹è¯•é›†: {len(sft_test)} æ¡")
    print(f"RL è®­ç»ƒé›†: {len(rl_train)} æ¡")
    print(f"RL æµ‹è¯•é›†: {len(rl_test)} æ¡")

    # ä¿å­˜
    os.makedirs("data/processed", exist_ok=True)

    def save_jsonl(data, path):
        with open(path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

    save_jsonl(sft_train, DATA_CONFIG.sft_data_path)
    save_jsonl(rl_train, DATA_CONFIG.rl_data_path)
    save_jsonl(sft_test + rl_test, DATA_CONFIG.test_data_path)  # æµ‹è¯•é›†åˆå¹¶

    # ä¿å­˜ç»Ÿè®¡
    with open("data/processed/filter_stats.json", 'w', encoding='utf-8') as f:
        json.dump(filter_obj.stats, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… æ•°æ®å·²ä¿å­˜:")
    print(f"   SFT è®­ç»ƒ: {DATA_CONFIG.sft_data_path}")
    print(f"   RL è®­ç»ƒ: {DATA_CONFIG.rl_data_path}")
    print(f"   æµ‹è¯•é›†: {DATA_CONFIG.test_data_path}")

    return len(sft_train), len(rl_train), len(sft_test)

if __name__ == "__main__":
    sft_count, rl_count, test_count = filter_data()
    print(f"\nâœ… æ•°æ®æ„å»ºå®Œæˆï¼")
    print(f"   æœ€ç»ˆæ•°æ®èµ„äº§ï¼šSFT {sft_count} æ¡ + RL {rl_count} æ¡ + æµ‹è¯• {test_count} æ¡")
    print(f"\nä¸‹ä¸€æ­¥ï¼šä¸Šä¼ åˆ°æœåŠ¡å™¨å¹¶å¼€å§‹è®­ç»ƒ")
```

---

## é˜¶æ®µ2ï¼šè®­ç»ƒè„šæœ¬å¼€å‘

ç”±äºè„šæœ¬è¾ƒé•¿ï¼Œå®Œæ•´ä»£ç å·²åœ¨å‰é¢ç»™å‡ºã€‚è¿™é‡Œæ€»ç»“å…³é”®ç‚¹ï¼š

### 2.1 SFT è®­ç»ƒï¼ˆscripts/4_train_sft.pyï¼‰

**å…³é”®ä¿®æ­£**ï¼š
1. âœ… ä½¿ç”¨ `BitsAndBytesConfig` è¿›è¡Œ 4bit é‡åŒ–
2. âœ… ä½¿ç”¨ `prepare_model_for_kbit_training`
3. âœ… æ•°æ®æ ¼å¼ä½¿ç”¨ Qwen çš„ chat template
4. âœ… ä½¿ç”¨ `SFTTrainer` çš„ `dataset_text_field` å‚æ•°

### 2.2 GRPO è®­ç»ƒï¼ˆscripts/5_train_grpo.pyï¼‰

**å…³é”®ä¿®æ­£**ï¼š
1. âš ï¸ **é‡è¦**ï¼šTRL çš„ `GRPOTrainer` reward å‡½æ•°ç­¾åéœ€è¦æ£€æŸ¥
2. âš ï¸ **é‡è¦**ï¼šéœ€è¦ç¡®è®¤æ˜¯å¦éœ€è¦ `AutoModelForCausalLMWithValueHead`
3. âœ… æ•°æ®æ ¼å¼éœ€è¦åŒ…å« `query` å­—æ®µ
4. âœ… reward å‡½æ•°è¿”å› `List[float]`

**ä¿®æ­£åçš„ reward å‡½æ•°ç­¾å**ï¼š

```python
def combined_reward(
    samples: List[str],          # ç”Ÿæˆçš„æ–‡æœ¬
    prompts: List[str],          # è¾“å…¥çš„ prompt
    outputs: List[str],          # å®Œæ•´è¾“å‡ºï¼ˆå¯é€‰ï¼‰
    **kwargs                     # é¢å¤–å‚æ•°ï¼ˆgold_answer, type ç­‰ï¼‰
) -> List[float]:
    """ç»„åˆå¥–åŠ±å‡½æ•°"""
    # ä» kwargs æå–å…ƒæ•°æ®
    gold_answers = kwargs.get("gold_answers", [])
    types = kwargs.get("types", [])

    # è®¡ç®—å¥–åŠ±
    format_rewards = compute_format_reward(samples)
    accuracy_rewards = compute_accuracy_reward(samples, gold_answers, types)

    # åŠ æƒç»„åˆ
    combined = [
        0.3 * f + 0.7 * a
        for f, a in zip(format_rewards, accuracy_rewards)
    ]

    return combined
```

---

## é˜¶æ®µ3ï¼šæœåŠ¡å™¨è®­ç»ƒæ‰§è¡Œ

### æ—¶é—´å’Œé¢„ç®—ä¿®æ­£

**AutoDL 5090 32GB å®é™…ä»·æ ¼**ï¼šçº¦ 3-4 å…ƒ/å°æ—¶ï¼ˆæŒ‰åœ°åŒºå’Œæœºå‹ä¸åŒï¼‰

| é˜¶æ®µ | æ—¶é—´ | è´¹ç”¨ï¼ˆæŒ‰ 3.5 å…ƒ/å°æ—¶ï¼‰ |
|------|------|----------------------|
| ç¯å¢ƒå‡†å¤‡ | 0.5h | ~2å…ƒ |
| SFT è®­ç»ƒ | 1.5h | ~5å…ƒ |
| GRPO è®­ç»ƒ | 2.5h | ~9å…ƒ |
| è¯„æµ‹ | 0.5h | ~2å…ƒ |
| éƒ¨ç½²æµ‹è¯• | 0.5h | ~2å…ƒ |
| **æ€»è®¡** | **5.5h** | **~20å…ƒ** |

**å‰©ä½™é¢„ç®—**ï¼š80 å…ƒï¼Œè¶³å¤Ÿå¤šæ¬¡å®éªŒå’Œè°ƒä¼˜ã€‚

### æœåŠ¡å™¨æ‰§è¡Œæµç¨‹

```bash
# 1. å¼€æœºï¼ˆé€‰æ‹© PyTorch 2.1 + CUDA 12.1 é•œåƒï¼‰
# 2. ä¸Šä¼ ä»£ç å’Œæ•°æ®
scp -r FTModel root@your_server_ip:/root/

# 3. å®‰è£…ä¾èµ–
cd /root/FTModel
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 4. æ‰§è¡Œè®­ç»ƒ
python scripts/4_train_sft.py 2>&1 | tee logs/sft_train.log
python scripts/5_train_grpo.py 2>&1 | tee logs/grpo_train.log

# 5. è¯„æµ‹
python scripts/6_evaluate.py 2>&1 | tee logs/eval.log

# 6. éƒ¨ç½²
python scripts/7_deploy.py --action merge
python scripts/7_deploy.py --action serve &
sleep 60
python scripts/7_deploy.py --action test
```

---

## é˜¶æ®µ4ï¼šè¯„æµ‹ä¸éƒ¨ç½²

ï¼ˆä»£ç å·²åœ¨å‰é¢ç»™å‡ºï¼Œè¿™é‡Œä¸é‡å¤ï¼‰

---

## é˜¶æ®µ5ï¼šé¢è¯•å‡†å¤‡

### 5.1 æ¶ˆèå®éªŒ

åˆ›å»º `scripts/8_ablation_study.py`ï¼ˆä»£ç å·²ç»™å‡ºï¼‰

### 5.2 é¢è¯•å±•ç¤ºææ–™æ¸…å•

1. **é¡¹ç›®æ¦‚è¿° PPT**ï¼ˆ3-5 é¡µï¼‰ï¼š
   - ç¬¬1é¡µï¼šé¡¹ç›®èƒŒæ™¯ï¼ˆFin-R1 è®ºæ–‡å¯å‘ï¼‰
   - ç¬¬2é¡µï¼šæŠ€æœ¯æ¶æ„å›¾
   - ç¬¬3é¡µï¼šæ•°æ®æ„å»ºæµç¨‹ï¼ˆåŒé‡è¿‡æ»¤ï¼‰
   - ç¬¬4é¡µï¼šè®­ç»ƒç­–ç•¥ï¼ˆSFT + GRPOï¼‰
   - ç¬¬5é¡µï¼šå®éªŒç»“æœï¼ˆæ¶ˆèå®éªŒå¯¹æ¯”ï¼‰

2. **ä»£ç ä»“åº“**ï¼šGitHub/Gitee å…¬å¼€ä»“åº“

3. **æŠ€æœ¯æ–‡æ¡£**ï¼š
   - README.mdï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰
   - æœ¬æ–‡æ¡£ï¼ˆå®Œæ•´æ‰§è¡Œè®¡åˆ’ï¼‰
   - API æ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰

4. **å®éªŒæŠ¥å‘Š**ï¼š
   - æ•°æ®è¿‡æ»¤æŠ¥å‘Šï¼ˆfilter_stats.jsonï¼‰
   - è®­ç»ƒæ›²çº¿å›¾ï¼ˆloss, reward, KLï¼‰
   - è¯„æµ‹ç»“æœï¼ˆeval_*.jsonï¼‰
   - æ¶ˆèå®éªŒå¯¹æ¯”è¡¨

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šAPI è°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**ï¼š`2_distill_data.py` æŠ¥é”™ `API key invalid`

**è§£å†³**ï¼š
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export API_KEY="your_deepseek_or_qwen_api_key"

# æˆ–åœ¨ configs/config.py ä¸­ç›´æ¥é…ç½®
API_CONFIG.api_key = "sk-xxxxx"
```

### é—®é¢˜2ï¼šæ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰

**ç—‡çŠ¶**ï¼šè®­ç»ƒæ—¶æŠ¥ `CUDA out of memory`

**è§£å†³**ï¼š
```python
# åœ¨ configs/config.py ä¸­è°ƒæ•´å‚æ•°
SFT_CONFIG.per_device_train_batch_size = 1  # å‡å° batch size
SFT_CONFIG.gradient_accumulation_steps = 16  # å¢å¤§æ¢¯åº¦ç´¯ç§¯
MODEL_CONFIG.model_max_length = 1024  # å‡å°åºåˆ—é•¿åº¦
```

### é—®é¢˜3ï¼šGRPO reward å‡½æ•°æŠ¥é”™

**ç—‡çŠ¶**ï¼š`reward_function returned wrong type`

**è§£å†³**ï¼šç¡®ä¿ reward å‡½æ•°è¿”å› `List[float]`ï¼š
```python
def my_reward(samples, **kwargs):
    rewards = [...]  # è®¡ç®—å¥–åŠ±
    return rewards  # å¿…é¡»æ˜¯ listï¼Œä¸æ˜¯ tensor
```

### é—®é¢˜4ï¼švLLM å¯åŠ¨å¤±è´¥

**ç—‡çŠ¶**ï¼š`ImportError: No module named 'vllm'`

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# æ ¹æ® CUDA ç‰ˆæœ¬å®‰è£…ï¼ˆç¤ºä¾‹ï¼šCUDA 12.1ï¼‰
pip install vllm==0.6.3

# å¦‚æœä»å¤±è´¥ï¼Œä½¿ç”¨ transformers æ¨ç†
python scripts/6_evaluate.py --use_transformers
```

### é—®é¢˜5ï¼šæ¨¡å‹è¾“å‡ºä¸åŒ…å«æ ‡ç­¾

**ç—‡çŠ¶**ï¼šè®­ç»ƒåæ¨¡å‹ä¸è¾“å‡º `<think><answer>`

**æ’æŸ¥**ï¼š
1. æ£€æŸ¥ SFT æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
2. å¢åŠ è®­ç»ƒæ­¥æ•°ï¼ˆè‡³å°‘ 500 æ­¥ï¼‰
3. æ£€æŸ¥ chat template æ˜¯å¦æ­£ç¡®åº”ç”¨
4. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„ç”Ÿæˆæ ·ä¾‹

---

## é¢è¯•é—®ç­”æ‰‹å†Œ

### æŠ€æœ¯æ·±åº¦ç±»

**Q1: ä¸ºä»€ä¹ˆéœ€è¦åŒé‡è¿‡æ»¤ï¼Ÿå•ä¸€æ‰“åˆ†ä¸è¡Œå—ï¼Ÿ**

A: åŒé‡è¿‡æ»¤æ˜¯ Fin-R1 çš„æ ¸å¿ƒè®¾è®¡ï¼š
1. **ç­”æ¡ˆæ­£ç¡®æ€§**æ˜¯ç¡¬çº¦æŸ - é‡‘èåœºæ™¯ä¸èƒ½å®¹å¿é”™è¯¯ç­”æ¡ˆï¼Œå¿…é¡»å…ˆä¿è¯ç­”æ¡ˆå¯¹
2. **æ¨ç†è´¨é‡**æ˜¯è½¯çº¦æŸ - åœ¨ç­”æ¡ˆå¯¹çš„åŸºç¡€ä¸Šï¼Œç­›é€‰æ¨ç†è¿‡ç¨‹æ¸…æ™°ã€é€»è¾‘è¿è´¯çš„æ ·æœ¬
3. å•ä¸€æ‰“åˆ†ä¼šæ··æ·†ä¸¤ä¸ªç»´åº¦ï¼Œå¯¼è‡´"ç­”æ¡ˆé”™ä½†æ¨ç†åƒæ¨¡åƒæ ·"çš„æ ·æœ¬é€šè¿‡

**Q2: SFT å’Œ GRPO çš„ç›®æ ‡å‡½æ•°åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**

A:
- **SFT ç›®æ ‡å‡½æ•°**ï¼š`max_Î¸ E_{(x,y)~D}[log P_Î¸(y|x)]`
  - æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶ï¼Œæœ¬è´¨æ˜¯è¡Œä¸ºå…‹éš†
  - è®©æ¨¡å‹å­¦ä¼š"å…ˆæ¨ç†å†ä½œç­”"çš„è¾“å‡ºç»“æ„

- **GRPO ç›®æ ‡å‡½æ•°**ï¼š`max_Î¸ E_x[E_{y~Ï€_Î¸}[R(x,y)]] - Î²Â·KL(Ï€_Î¸ || Ï€_ref)`
  - æœ€å¤§åŒ–å¥–åŠ±æœŸæœ›ï¼ŒåŒæ—¶ç”¨ KL æ•£åº¦çº¦æŸæ¨¡å‹ä¸è¦åç¦» SFT åˆå§‹åŒ–å¤ªè¿œ
  - Î² æ˜¯æƒè¡¡ç³»æ•°ï¼Œé˜²æ­¢ reward hacking

**Q3: GRPO å’Œ PPO æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

A:
- **PPO**: ç”¨å…¨å±€ baselineï¼ˆé€šå¸¸æ˜¯ value network çš„è¾“å‡ºï¼‰è®¡ç®—ä¼˜åŠ¿å‡½æ•°
- **GRPO**: ç”¨åŒä¸€ä¸ª prompt çš„å¤šä¸ªé‡‡æ ·çš„**ç»„å†…ç›¸å¯¹è¡¨ç°**è®¡ç®—ä¼˜åŠ¿
  - ä¼˜åŠ¿ï¼šä¸éœ€è¦ç‹¬ç«‹è®­ç»ƒ value networkï¼Œæ›´ç¨³å®š
  - åŸç†ï¼š`A(y) = R(y) - mean(R(y_1, ..., y_k))`ï¼Œå…¶ä¸­ y_1...y_k æ˜¯åŒä¸€ prompt çš„å¤šä¸ªé‡‡æ ·

**Q4: ä¸ºä»€ä¹ˆæ ¼å¼å¥–åŠ±å’Œå‡†ç¡®æ€§å¥–åŠ±è¦åˆ†å¼€ï¼Ÿ**

A:
1. **è§£è€¦å…³æ³¨ç‚¹**ï¼šæ ¼å¼ä¿è¯å¯è§£é‡Šæ€§ï¼Œå‡†ç¡®æ€§ä¿è¯ä¸šåŠ¡ä»·å€¼
2. **ä¸åŒéš¾åº¦**ï¼šæ ¼å¼å¥–åŠ±å®¹æ˜“å­¦ï¼ˆè§„åˆ™æ˜ç¡®ï¼‰ï¼Œå‡†ç¡®æ€§å¥–åŠ±éš¾å­¦ï¼ˆéœ€è¦æ¨ç†èƒ½åŠ›ï¼‰
3. **å¯è°ƒæƒé‡**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ï¼Œæ¯”å¦‚ç›‘ç®¡åœºæ™¯æé«˜æ ¼å¼æƒé‡ï¼Œä¸šåŠ¡åœºæ™¯æé«˜å‡†ç¡®æ€§æƒé‡
4. **Fin-R1 çš„å®éªŒ**ï¼šå•ç‹¬ç”¨æ ¼å¼å¥–åŠ±ä¼šå¯¼è‡´"æ ¼å¼å¯¹ä½†èƒ¡è¯´å…«é“"

### å·¥ç¨‹å®è·µç±»

**Q5: å¦‚æœæ•°æ®è’¸é¦æˆæœ¬å¤ªé«˜æ€ä¹ˆåŠï¼Ÿ**

A:
1. **çŸ­æœŸæ–¹æ¡ˆ**ï¼š
   - ç”¨å¼€æº CoT æ•°æ®é›†ï¼ˆå¦‚ MetaMath, OpenO1ï¼‰æ”¹é€ 
   - ç”¨æ›´ä¾¿å®œçš„ APIï¼ˆDeepSeek 0.14 å…ƒ/M tokensï¼‰
   - å‡å°‘æ•°æ®é‡ï¼ˆ100-200 æ¡å°±èƒ½è·‘é€šé“¾è·¯ï¼‰

2. **é•¿æœŸæ–¹æ¡ˆ**ï¼š
   - è‡ªå·±è®­ç»ƒä¸€ä¸ª 7B æ•™å¸ˆæ¨¡å‹ï¼ˆä¸€æ¬¡æŠ•å…¥ï¼Œåå¤ä½¿ç”¨ï¼‰
   - ç”¨è’¸é¦åçš„æ¨¡å‹åšè‡ªæˆ‘è¿­ä»£ï¼ˆself-trainingï¼‰
   - äººå·¥ç¼–å†™é«˜è´¨é‡ CoT æ¨¡æ¿

**Q6: å¦‚ä½•æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒï¼Ÿ**

A:
1. **æ•°æ®è§„æ¨¡**ï¼šæ‰©å±•åˆ° 10 ä¸‡+ æ¡ï¼Œè¦†ç›–æ›´å¤šé‡‘èåœºæ™¯
2. **æ¨¡å‹é€‰å‹**ï¼šç”¨ 7B/14B åŸºåº§ï¼Œæ€§èƒ½æ›´å¥½
3. **è¯„æµ‹ä½“ç³»**ï¼š
   - ç¦»çº¿è¯„æµ‹ï¼šå¤šä¸ªé‡‘èåŸºå‡†æµ‹è¯•
   - åœ¨çº¿è¯„æµ‹ï¼šA/B æµ‹è¯•ã€ç”¨æˆ·æ»¡æ„åº¦
   - äººå·¥æŠ½æ ·ï¼šå®šæœŸäººå·¥å®¡æ ¸ç”Ÿæˆè´¨é‡
4. **éƒ¨ç½²ä¼˜åŒ–**ï¼š
   - å¤šå¡å¹¶è¡Œï¼ˆtensor parallelismï¼‰
   - è´Ÿè½½å‡è¡¡ï¼ˆå¤šå®ä¾‹ + Nginxï¼‰
   - ç›‘æ§å‘Šè­¦ï¼ˆPrometheus + Grafanaï¼‰
5. **æŒç»­è¿­ä»£**ï¼šæ”¶é›†çº¿ä¸Šåé¦ˆï¼ŒæŒç»­ä¼˜åŒ–æ•°æ®å’Œæ¨¡å‹

**Q7: å¦‚ä½•é¿å…æ•°æ®æ³„æ¼ï¼Ÿ**

A:
1. **ä¸¥æ ¼åˆ‡åˆ†**ï¼šè®­ç»ƒ/æµ‹è¯•é›†æŒ‰ ID æˆ–æ—¶é—´åˆ‡åˆ†ï¼Œä¸èƒ½éšæœºæ‰“ä¹±ååˆ‡åˆ†
2. **ç‹¬ç«‹æµ‹è¯•é›†**ï¼šæµ‹è¯•é›†ä¸å‚ä¸ä»»ä½•è®­ç»ƒè¿‡ç¨‹ï¼ˆåŒ…æ‹¬è¶…å‚è°ƒä¼˜ï¼‰
3. **å®šæœŸæ›´æ–°**ï¼šå®šæœŸæ›´æ–°æµ‹è¯•é›†ï¼Œé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆæ—§æµ‹è¯•é›†
4. **éšç§ä¿æŠ¤**ï¼šå¦‚æœç”¨çœŸå®ä¸šåŠ¡æ•°æ®ï¼Œéœ€è¦è„±æ•å¤„ç†

### ä¸šåŠ¡ç†è§£ç±»

**Q8: ä¸ºä»€ä¹ˆé‡‘èåœºæ™¯éœ€è¦ CoTï¼Ÿ**

A:
1. **ç›‘ç®¡è¦æ±‚**ï¼šé‡‘èæ¨¡å‹éœ€è¦å¯è§£é‡Šæ€§ï¼Œç›‘ç®¡éƒ¨é—¨è¦æ±‚"é»‘ç›’"æ¨¡å‹ç»™å‡ºæ¨ç†ä¾æ®
2. **ç”¨æˆ·ä¿¡ä»»**ï¼šç”¨æˆ·çœ‹åˆ°è®¡ç®—æ­¥éª¤æ›´å®¹æ˜“ä¿¡ä»»ç»“æœï¼ˆå°¤å…¶æ˜¯é‡‘é¢è®¡ç®—ï¼‰
3. **å¯è°ƒè¯•æ€§**ï¼šå‡ºé”™æ—¶å¯ä»¥å®šä½åˆ°å…·ä½“æ¨ç†ç¯èŠ‚ï¼Œå¿«é€Ÿä¿®å¤
4. **å¯éªŒè¯æ€§**ï¼šäººå·¥å®¡æ ¸æ—¶å¯ä»¥æ£€æŸ¥æ¨ç†é€»è¾‘ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆ

**Q9: å¦‚ä½•è¯„ä¼°æ¨¡å‹åœ¨çœŸå®ä¸šåŠ¡ä¸­çš„è¡¨ç°ï¼Ÿ**

A:
1. **ç¦»çº¿æŒ‡æ ‡**ï¼š
   - æ ¼å¼æ­£ç¡®ç‡ï¼ˆå¯è§£é‡Šæ€§ï¼‰
   - ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆä¸šåŠ¡ä»·å€¼ï¼‰
   - æŒ‰ä»»åŠ¡ç±»å‹ç»†åˆ†ï¼ˆå‘ç°çŸ­æ¿ï¼‰

2. **åœ¨çº¿æŒ‡æ ‡**ï¼š
   - ç”¨æˆ·æ»¡æ„åº¦ï¼ˆæ˜¾å¼åé¦ˆï¼šç‚¹èµ/ç‚¹è¸©ï¼‰
   - ä¸šåŠ¡è½¬åŒ–ç‡ï¼ˆç”¨æˆ·æ˜¯å¦é‡‡çº³æ¨¡å‹å»ºè®®ï¼‰
   - é”™è¯¯ç‡ï¼ˆäººå·¥å®¡æ ¸å‘ç°çš„é”™è¯¯æ¯”ä¾‹ï¼‰

3. **A/B æµ‹è¯•**ï¼š
   - å¯¹æ¯”æ–°æ¨¡å‹ vs æ—§æ¨¡å‹
   - å¯¹æ¯”æ¨¡å‹ vs äººå·¥ä¸“å®¶
   - å¯¹æ¯”ä¸åŒ prompt ç­–ç•¥

---

## æ€»ç»“ï¼šäº¤ä»˜ç‰©æ£€æŸ¥æ¸…å•

- [ ] ä»£ç ä»“åº“ï¼ˆGitHub/Giteeï¼‰
- [ ] å®Œæ•´æ•°æ®æµæ°´çº¿è„šæœ¬ï¼ˆ1-3ï¼‰
- [ ] è®­ç»ƒè„šæœ¬ï¼ˆ4-5ï¼‰
- [ ] è¯„æµ‹å’Œéƒ¨ç½²è„šæœ¬ï¼ˆ6-7ï¼‰
- [ ] æ¶ˆèå®éªŒè„šæœ¬ï¼ˆ8ï¼‰
- [ ] é…ç½®æ–‡ä»¶ï¼ˆconfigs/config.pyï¼‰
- [ ] ä¾èµ–æ–‡ä»¶ï¼ˆrequirements.txtï¼‰
- [ ] é¡¹ç›®æ–‡æ¡£ï¼ˆREADME + æœ¬æ–‡æ¡£ï¼‰
- [ ] å®éªŒæŠ¥å‘Šï¼š
  - [ ] æ•°æ®è¿‡æ»¤æŠ¥å‘Šï¼ˆfilter_stats.jsonï¼‰
  - [ ] è®­ç»ƒæ—¥å¿—ï¼ˆlogs/*.logï¼‰
  - [ ] è¯„æµ‹ç»“æœï¼ˆreports/eval_*.jsonï¼‰
  - [ ] æ¶ˆèå®éªŒï¼ˆreports/ablation_summary.jsonï¼‰
- [ ] é¢è¯•ææ–™ï¼š
  - [ ] é¡¹ç›®æ¦‚è¿° PPTï¼ˆ3-5 é¡µï¼‰
  - [ ] æŠ€æœ¯é—®ç­”å‡†å¤‡ï¼ˆæœ¬æ–‡æ¡£ç¬¬9èŠ‚ï¼‰
  - [ ] Demo è§†é¢‘/æˆªå›¾ï¼ˆå¯é€‰ï¼‰

---

## ç«‹å³å¼€å§‹

```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export API_KEY="your_api_key_here"

# 2. åˆ›å»ºé¡¹ç›®ç»“æ„
cd c:\gitclones\FTModel
mkdir -p data/{raw,processed} scripts ckpts reports configs logs

# 3. å¤åˆ¶æœ¬æ–‡æ¡£ä¸­çš„æ‰€æœ‰è„šæœ¬åˆ°å¯¹åº”ç›®å½•

# 4. æœ¬åœ°æ‰§è¡Œæ•°æ®æ„å»ºï¼ˆ3-4 å°æ—¶ï¼‰
python scripts/1_prepare_raw_data.py
python scripts/2_distill_data.py
python scripts/3_filter_data.py

# 5. ä¸Šä¼ åˆ° AutoDL å¹¶æ‰§è¡Œè®­ç»ƒï¼ˆ5-6 å°æ—¶ï¼‰
# ï¼ˆè§"é˜¶æ®µ3ï¼šæœåŠ¡å™¨è®­ç»ƒæ‰§è¡Œ"ï¼‰

# 6. æ•´ç†é¢è¯•ææ–™ï¼ˆ1 å°æ—¶ï¼‰
```

**é¢„è®¡æ€»æ—¶é—´**ï¼š10-12 å°æ—¶
**é¢„è®¡æ€»è´¹ç”¨**ï¼šAPI è´¹ 5 å…ƒ + æœåŠ¡å™¨ 20 å…ƒ = **25 å…ƒ**

**å‰©ä½™é¢„ç®—å……è£•**ï¼Œå¯ä»¥å¤šæ¬¡å®éªŒå’Œè°ƒä¼˜ï¼

---

**Good Luck! ğŸš€**
